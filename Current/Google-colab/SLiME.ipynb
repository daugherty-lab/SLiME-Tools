{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlK2Z4raaKDT"
      },
      "source": [
        "**Important notes:** This notebook pipeline only works with protein sequences (no headers) or uploads of zipped fasta files (with headers). To upload zipped fasta files, click the \"File\" icon on the left panel, and click the \"Upload to session storage\" icon. These files will be erased at the end of each session, or after the time limit imposed by Google Colaboratory (~12 hours).\n",
        "\n",
        "I had intended to make a dropdown menu for downloadable species. Unfortunately Google Colab makes this challenging. Instead, search here by **species name** to see if the genome is available [here](https://colab.research.google.com/drive/168xYMKDXyaDP9jqINjQC38z6R9dfNotn#scrollTo=VFfNMG873Piv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJzMiatY-IFn",
        "outputId": "959fb892-f605-4493-d170-26cd27659251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "#@title <- Click this to install dependencies before reading the info below. (~3-6min)\n",
        "#@markdown The kernel MUST restart for you to proceed with the pipeline.\n",
        "\n",
        "# download prebuilt motif files\n",
        "# Does this replace the file everytime this cell is run?\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UBvY0MCEinCZi_VuP1CqXAHnn6nURjpH' -O Tsu-and-Beierschmitt-2021.txt\n",
        "\n",
        "!wget -q 'https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/assembly_summary.txt' -O mammal_assembly_summary.txt\n",
        "\n",
        "# install conda package manager\n",
        "# For more information: https://docs.conda.io/en/latest/\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -y -c bioconda meme=4.11.2 &>/dev/null #5.3.0\n",
        "!conda install -y -c conda-forge biopython &>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnYPjw_pQ_2V"
      },
      "source": [
        "# How to run this pipeline:\n",
        "1. Upload any zipped custom fasta-formatted sequences (gaps are ok), zipped FUBAR posterior.csv files, and prebuilt db.csv. If you are unsure about whether these file formats match, please use the input checker notebook [**In Progress**].<br>\n",
        "2. Then, set inputs (1. Set inputs).<br>\n",
        "* If you uploaded custom fasta seqs/alns (seqs.zip), fill checkbox \"Upload_zipped_fastas\"\n",
        "* If you selected custom motif options, see Step 3. and then follow ### prompt to upload the MEME-formatted motif file. <br>\n",
        "* If you uploaded custom FUBAR posterior.csv files (posteriors.zip), fill checkbox \"enable PSRS\"\n",
        "* If you uploaded a custom orthogonal dataset to merge to your motif results (db.csv), fill checkbox \"enable prebuiltdb\" \n",
        "<br>\n",
        "<br>\n",
        "3. Click \"Runtime\" from the top menu and \"Run all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYkK7QfP5KCg",
        "outputId": "98805d43-e9b5-42ed-c84e-f98c3c0aad1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zipped file(s) found: whole_seqs_AA_12_seqs.zip\n",
            "Setting up job: primate_seqs_0b0c1\n",
            "Including PSR file(s) to be merged.\n",
            "Including prebuiltdb file(s) to be merged.\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Set inputs\n",
        "\n",
        "# =====================================================\n",
        "# import libraries\n",
        "# =====================================================\n",
        "import argparse\n",
        "import glob2\n",
        "import hashlib\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "\n",
        "from google.colab import files\n",
        "from time import perf_counter\n",
        "\n",
        "# =====================================================\n",
        "# User-define functions\n",
        "# =====================================================\n",
        "def add_hash(x,y):\n",
        "  '''\n",
        "  Give jobname unique identifier\n",
        "  '''\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "# =====================================================\n",
        "# Set inputs from google form\n",
        "# =====================================================\n",
        "#@markdown Fill in or select one of the three:\n",
        "RefSeq_Species = '' #@param {type:\"string\"}\n",
        "#@markdown OR\n",
        "# Acquire user input query sequence\n",
        "query_sequence = '' #@param {type:\"string\"}\n",
        "#@markdown OR\n",
        "\n",
        "# Regardless of whether you've input a sequence or not\n",
        "# Acquire boolean (checkbox) value for zipped fasta\n",
        "Upload_zipped_fastas = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Acquire user input jobname\n",
        "jobname = 'primate_seqs' #@param {type:\"string\"}\n",
        "\n",
        "# Acquire user selected drop-down motif option\n",
        "motif_options = \"Tsu-and-Beierschmitt-2021 (Rec pval: 0.00581)\" #@param [\"none\", \"Tsu-and-Beierschmitt-2021 (Rec pval: 0.00581)\",\"custom\"]\n",
        "#@markdown * Warning: Do not select more than one motif upload.\n",
        "\n",
        "# Acquire user input pval number\n",
        "pval = 0.00581 #@param {type:\"number\"}\n",
        "#@markdown * This must be a numerical value between 0 and 1.\n",
        "\n",
        "# Acquire boolean check for pos. sel. residue (PSR) files\n",
        "enable_PSRs = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Acquire boolean check for prebuilt db .csv\n",
        "enable_prebuiltdb = True #@param {type:\"boolean\"}\n",
        "\n",
        "# =====================================================\n",
        "# Set input sequence source\n",
        "# =====================================================\n",
        "\n",
        "is_input = False\n",
        "\n",
        "is_RefSeq_Species = False\n",
        "# Refseq proteome\n",
        "if RefSeq_Species:\n",
        "  is_input = True\n",
        "  # Create dataframe from summary file\n",
        "  infile_ind = [7, 15, 19] # 'organism_name', 'asm_name', 'ftp_path'\n",
        "  mammal_df = pd.read_csv(\n",
        "      \"mammal_assembly_summary.txt\", \n",
        "      sep = '\\t',\n",
        "      skiprows=1,\n",
        "      usecols = infile_ind\n",
        "  )\n",
        "  mammal_match = mammal_df[mammal_df['organism_name']==RefSeq_Species]\n",
        "  if len(mammal_match) == 1:\n",
        "      print(f\"Selecting RefSeq organism: {RefSeq_Species}\")\n",
        "      matched_asm_name = mammal_match['asm_name'].iloc[0]\n",
        "      matched_ftp = mammal_match['ftp_path'].iloc[0]\n",
        "      ftp_base = matched_ftp.split(\"/\")[-1].split('protein')[0]\n",
        "      proteome_ftp = f\"{matched_ftp}/{ftp_base}_protein.faa.gz\"\n",
        "      is_RefSeq_Species = True\n",
        "  else:\n",
        "      print(\"Check for the species name in Check_Inputs.ipynb.\")\n",
        "      is_RefSeq_Species = False\n",
        "      \n",
        "# user-inputted query\n",
        "query_sequence = \"\".join(re.split('; |, |\\*|\\n|-| ', query_sequence)).upper() # remove whitespaces\n",
        "if query_sequence:\n",
        "  is_input = True\n",
        "  non_AAs = {'O', 'U', 'X', 'B', 'Z', 'J'}\n",
        "  if non_AAs.intersection(query_sequence): \n",
        "    print(\"Warning: Invalid AA(s) in sequence.\")\n",
        "else:\n",
        "  query_sequence = \"empty\"\n",
        "\n",
        "if Upload_zipped_fastas:\n",
        "  seq_zipfiles = glob2.glob(\"*seqs.zip\")\n",
        "  if seq_zipfiles:\n",
        "    is_input = True\n",
        "    query_sequence = \"empty\"\n",
        "    print(\"Zipped file(s) found:\", *seq_zipfiles)\n",
        "  else:\n",
        "    print(\"Zipped file(s) not found. Try reuploading with a seqs.zip suffix.\")\n",
        "\n",
        "# =====================================================\n",
        "# Create job ID and associated folder\n",
        "# =====================================================\n",
        "\n",
        "if is_input:\n",
        "  # Acquire user input jobname and add unique identifier\n",
        "  basejobname = \"\".join(jobname.split()) # remove whitespaces\n",
        "  basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "  jobname = add_hash(basejobname, query_sequence)\n",
        "  while os.path.isfile(f\"{jobname}.csv\"):\n",
        "    jobname = add_hash(basejobname, ''.join(random.sample(query_sequence,len(query_sequence))))\n",
        "  with open(f\"{jobname}.csv\", \"w\") as text_file:\n",
        "    text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "  print(\"Setting up job:\", jobname)\n",
        "\n",
        "  # Placeholder - this will be the final output\n",
        "  os.mkdir(jobname)\n",
        "  # queries_path=f\"{jobname}/{jobname}.csv\"\n",
        "else:\n",
        "  print(\"No input sequences found.\")\n",
        "\n",
        "# =====================================================\n",
        "# Check motif input\n",
        "# =====================================================\n",
        "\n",
        "# Check for motif input\n",
        "if motif_options == \"Tsu-and-Beierschmitt-2021 (Rec pval: 0.00581)\":\n",
        "  use_motif = True\n",
        "  custom_motif_path = \"/content/\"\n",
        "  motif = f\"{custom_motif_path}Tsu-and-Beierschmitt-2021.txt\"\n",
        "elif motif_options == \"custom\":\n",
        "  print(\"Input MEME-formatted motif .txt file:\")\n",
        "  custom_motif_path = f\"{jobname}/motif/\"\n",
        "  os.mkdir(custom_motif_path)\n",
        "  uploaded_motif = files.upload()\n",
        "  use_motif = True\n",
        "  motif = \"\"\n",
        "  for fn in uploaded_motif.keys():\n",
        "    motif = f\"{custom_motif_path}{fn}\"\n",
        "    os.rename(fn, motif)\n",
        "else:\n",
        "  print(\"Please select/upload a motif.\")\n",
        "  custom_motif_path = None\n",
        "  use_motif = False\n",
        "\n",
        "# =====================================================\n",
        "# Check other inputs\n",
        "# =====================================================\n",
        "\n",
        "# check for PSR input\n",
        "is_PSRs = False\n",
        "if enable_PSRs:\n",
        "  PSRs = glob2.glob(\"*posteriors.zip\")\n",
        "  if PSRs:\n",
        "    is_PSRs = True\n",
        "  else:\n",
        "    print(\"PSR file(s) not found. Skipping PSRs for now. Try reuploading with a posteriors.zip suffix.\")\n",
        "\n",
        "# check for uploaded, prebuilt db\n",
        "if enable_prebuiltdb:\n",
        "  dbs = glob2.glob(\"*db.csv\")\n",
        "  use_db = True\n",
        "  prebuiltdb = \"\"\n",
        "  for db in dbs:\n",
        "    prebuiltdb = db\n",
        "  if prebuiltdb:\n",
        "    is_prebuiltdb = True\n",
        "else:\n",
        "  is_prebuiltdb = False\n",
        "  use_db = False\n",
        "\n",
        "# =====================================================\n",
        "# Check overriding priorities\n",
        "# =====================================================\n",
        "\n",
        "# only include PSRs if user set uploaded files\n",
        "if is_PSRs and query_sequence == \"empty\" and not is_RefSeq_Species:\n",
        "  print(\"Including PSR file(s) to be merged.\")\n",
        "else:\n",
        "  is_PSRs = False\n",
        "\n",
        "# only include prebuilt db if user set uploaded files\n",
        "if is_prebuiltdb and query_sequence == \"empty\" and not is_RefSeq_Species:\n",
        "  print(\"Including prebuiltdb file(s) to be merged.\")\n",
        "else:\n",
        "  custom_prebuiltdb_path = None\n",
        "  use_db = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9kPocI-RGB5",
        "outputId": "a84703b9-c78c-4c0e-9038-a636d0785bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed time (seconds): 7.9033499110000776\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Build sequences and related files\n",
        "t1_start = perf_counter()\n",
        "\n",
        "# Make input directory\n",
        "custom_seq_path = f\"{jobname}/seqs\"\n",
        "os.mkdir(custom_seq_path)\n",
        "\n",
        "# Create bash script to split multifasta into singular fastas\n",
        "fa_sh = \"\"\"\n",
        "#!/bin/zsh\n",
        "\n",
        "# \"$#\" = value of the total number of command line args passed\n",
        "# As long as \"$#\" is greater than (-gt) 0 args, keep while loop alive\n",
        "while [[ \"$#\" -gt 0 ]]; do\n",
        "    # Check each case (options/flags) until match is found\n",
        "    case $1 in\n",
        "        # get input following arg option, then shift to next str\n",
        "        -i|--inputfa) inputfa=\"$2\"; shift ;;\n",
        "        -s|--suffix) suffix=\"$2\"; shift ;;\n",
        "        -o|--outdir) outdir=\"$2\"; shift ;;\n",
        "        \n",
        "        # if extra, unmatched options show up, exit\n",
        "        # Exit code 0 - Success\n",
        "        # Exit code 1 - General/misc errors, such as \"divide by zero\" and other impermissible operations\n",
        "        *) echo \"Unknown parameter passed: $1\"; exit 1 ;;\n",
        "    \n",
        "    # end case search (case spelled backwards)\n",
        "    esac\n",
        "    shift # to the next str, if any, then loop\n",
        "done\n",
        "\n",
        "while read line\n",
        "do\n",
        "    if [[ ${line:0:1} == \">\" ]]\n",
        "    then\n",
        "        outbase_headless=${line#\">\"}\n",
        "        outbase_tailless=${outbase_headless%\"$suffix\"}\n",
        "        outfile=\"$outdir/${outbase_tailless}\"\n",
        "        echo $line > $outfile\n",
        "    else\n",
        "        echo $line >> $outfile\n",
        "    fi\n",
        "done < $inputfa\n",
        "\"\"\"\n",
        "with open('fa_filter.sh', 'w') as file:\n",
        "  file.write(fa_sh)\n",
        "\n",
        "# Create .fa for RefSeq query and send to directory\n",
        "if is_RefSeq_Species:\n",
        "    orig_fapath = f\"{jobname}/proteome.faa\"\n",
        "    !wget -q $proteome_ftp -O proteome.faa.gz\n",
        "    !gunzip -d /content/proteome.faa.gz\n",
        "    !mv /content/proteome.faa $orig_fapath\n",
        "\n",
        "    # import biopython to format seq names\n",
        "    from Bio import SeqIO\n",
        "    \n",
        "    # Referencing the original fasta, create new fasta with new headers\n",
        "    print(f\"Building {matched_asm_name} proteome fastas...\")\n",
        "    corrected_fapath = f\"{jobname}/corrected_proteome.faa\"\n",
        "    with open(orig_fapath) as original_faa:\n",
        "        with open(corrected_fapath, 'w') as corrected_faa:\n",
        "            records = SeqIO.parse(original_faa, 'fasta')\n",
        "            for record in records:\n",
        "                record.id = f\"{record.id}.12taxa.fa_{matched_asm_name}\"\n",
        "                record.description = \"\"\n",
        "                SeqIO.write(record, corrected_faa, 'fasta')\n",
        "    !bash fa_filter.sh -i $corrected_fapath -o $custom_seq_path -s \"_\"$matched_asm_name\n",
        "    \n",
        "\n",
        "# Create .fa for input query and send to directory\n",
        "if query_sequence != \"empty\":\n",
        "  with open(f\"{custom_seq_path}/{jobname}.12taxa.fa\", 'w') as inputfa:\n",
        "    inputfa.write(f\">{jobname}.12taxa.fa_QuerySeq\\n\")\n",
        "    inputfa.write(f\"{query_sequence}\\n\")\n",
        "\n",
        "# Unzip very quietly (qq) and send .fa files to directory\n",
        "if Upload_zipped_fastas:\n",
        "  for seq_zip in seq_zipfiles:\n",
        "    !unzip -qq -j $seq_zip -d $custom_seq_path\n",
        "\n",
        "# Unzip very quietly (qq) and send .posterior.csv files to directory\n",
        "if is_PSRs:\n",
        "  custom_PSR_path = f\"{jobname}/posteriors\"\n",
        "  os.mkdir(custom_PSR_path)\n",
        "  for PSR in PSRs:\n",
        "    !unzip -qq -j $PSR -d $custom_PSR_path\n",
        "else:\n",
        "  custom_PSR_path = \"\"\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time (seconds):\", t1_stop-t1_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezIexprMSleg",
        "outputId": "7c266bdc-4b9e-4742-99c1-4a4f62559212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating FIMO output directory.\n",
            "Writing files to: primate_seqs_0b0c1/FIMO_out.\n",
            "Running iterative FIMO...\n",
            "iterative_FIMO completed.\n",
            "Cleaning empty FIMO .tsv files...\n",
            "Directory cleaned.\n",
            "Elapsed time (seconds): 533.1267852909996\n"
          ]
        }
      ],
      "source": [
        "#@title 3. Experimental - FIMO Search motif against all inputs (~10min)\n",
        "#@markdown parallelized across ~2 processes given by colab\n",
        "t1_start = perf_counter()\n",
        "\n",
        "# Create directory for FIMO output\n",
        "print(f\"Creating FIMO output directory.\")\n",
        "custom_FIMO_path = f\"{jobname}/FIMO_out\"\n",
        "os.mkdir(custom_FIMO_path)\n",
        "print(f\"Writing files to: {custom_FIMO_path}.\")\n",
        "\n",
        "# Create bash script to run FIMO on every file in specified directory\n",
        "sh = \"\"\"\n",
        "#!/bin/zsh\n",
        "\n",
        "# \"$#\" = value of the total number of command line args passed\n",
        "# As long as \"$#\" is greater than (-gt) 0 args, keep while loop alive\n",
        "while [[ \"$#\" -gt 0 ]]; do\n",
        "    # Check each case (options/flags) until match is found\n",
        "    case $1 in\n",
        "        # get input following arg option, then shift to next str\n",
        "        -i|--inputdir) inputdir=\"$2\"; shift ;;\n",
        "        -m|--motif) motif=\"$2\"; shift ;;\n",
        "        -p|--pval) pval=\"$2\"; shift ;;\n",
        "        -o|--oc) outputdir=\"$2\"; shift ;;\n",
        "        \n",
        "        # if extra, unmatched options show up, exit\n",
        "        # Exit code 0 - Success\n",
        "        # Exit code 1 - General/misc errors, such as \"divide by zero\" and other impermissible operations\n",
        "        *) echo \"Unknown parameter passed: $1\"; exit 1 ;;\n",
        "    \n",
        "    # end case search (case spelled backwards)\n",
        "    esac\n",
        "    shift # to the next str, if any, then loop\n",
        "done\n",
        "N=4\n",
        "(\n",
        "for f in $inputdir/*.fa\n",
        "do\n",
        "    ((i=i%N)); ((i++==0)) && wait\n",
        "    # get basename of filepath\n",
        "    baseFname=$(basename $f .fa)\n",
        "    # FIMO on each file\n",
        "    fimo --oc \"$outputdir\" --verbosity 1 --text --thresh $pval --max-stored-scores 8000000 \"$motif\" \"$f\" > \"$outputdir/$baseFname\"\"_fimo.tsv\" &\n",
        "done\n",
        ")\n",
        "\"\"\"\n",
        "with open('parallel_FIMO.sh', 'w') as file:\n",
        "  file.write(sh)\n",
        "\n",
        "# Run parallel FIMO script\n",
        "print(f\"Running iterative FIMO...\")\n",
        "!bash parallel_FIMO.sh -i $custom_seq_path -m $motif -p $pval -o $custom_FIMO_path &>/dev/null\n",
        "print(f\"iterative_FIMO completed.\")\n",
        "\n",
        "# Delete empty .tsv files created by FIMO\n",
        "print(f\"Cleaning empty FIMO .tsv files...\")\n",
        "!find $custom_FIMO_path -size 0 -print -delete &>/dev/null\n",
        "print(f\"Directory cleaned.\")\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time (seconds):\", t1_stop-t1_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cum25OU5XgTM",
        "outputId": "566c5371-ad81-46e0-dfdf-1becbd8044ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed time (seconds): 1394.0104284500003\n"
          ]
        }
      ],
      "source": [
        "#@title 4. Summarize all FIMO .tsv inputs (~15-25min)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class summary_args:\n",
        "    csv_out: str\n",
        "    fimo_tsvs: List[str]\n",
        "    fasta_dir: str\n",
        "    PSG_dir: str\n",
        "    def __post_init__(self):\n",
        "        if not self.PSG_dir:\n",
        "            self.PSG_dir = 'skip'\n",
        "\n",
        "def glob_files(path: str) -> List[str]:\n",
        "    return(glob2.glob(f'{path}/*.tsv'))\n",
        "\n",
        "def expand_motif_aln(aln_dir: str, in_seqID: str, seq_sites: List[int], motif_length = int): # Need correct typehint -> (int, List[str]):\n",
        "    '''Default shows only the species sequences with motif hits.\n",
        "    When an aln dir is specified, this function is invoked to\n",
        "    show the motif alignment across both hits and nonhits.'''\n",
        "    from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "\n",
        "    # collect species name from title and relevant motif alignment from sequence\n",
        "    with open(f'{aln_dir}/{in_seqID}.12taxa.fa') as aln_file:\n",
        "        species_regions = []\n",
        "        seq_length = 0\n",
        "        for title, sequence in SimpleFastaParser(aln_file):\n",
        "            species_name = title.split('_')[-1]\n",
        "            if species_name == 'hg38' or species_name == 'QuerySeq':\n",
        "                seq_length = len(sequence.replace('-',''))\n",
        "            species_regions.append([f'{species_name}: {sequence[pos-1:pos+motif_length-1]}' for pos in seq_sites])\n",
        "    return(seq_length, list(map(list, zip(*species_regions))))\n",
        "\n",
        "def map_PSRs(indir: str, in_seqID: str, seq_sites: List[int]) -> List[List[str]]:\n",
        "    '''Returns stringmap of Positive Selection at Residues (PSRs) from dir of FUBAR files, \n",
        "    if relevant to the motif range (pos-1:pos+7). PSRs are recorded as '+', \n",
        "    and non-PSRs are recorded as '-'.'''\n",
        "    site_map = [list(range(pos-1,pos+7)) for pos in seq_sites]\n",
        "    PSRs = []\n",
        "    try: # File may or may not exist, but if it does, collect PSR entries\n",
        "        with open(f'{indir}/{in_seqID}.12taxa.fa.12taxon.tree.grid_info.posteriors.csv') as PSG_file:\n",
        "            next(PSG_file) # Skip first line\n",
        "            for line in PSG_file:\n",
        "                PSR = int(line.split('0.')[0])\n",
        "                PSRs.append(PSR)\n",
        "        \n",
        "        # map sites with presence '+' or absence '_' of PSR\n",
        "        for site_i, site in enumerate(site_map):\n",
        "            for pos_i, pos in enumerate(site):\n",
        "                if pos in PSRs:\n",
        "                    site_map[site_i][pos_i] = f'{pos}'\n",
        "                else:\n",
        "                    site_map[site_i][pos_i] = '-'\n",
        "            site_map[site_i] = ''.join(site)\n",
        "        return(site_map)\n",
        "    except IOError: #if file doesn't exist, return default string\n",
        "        return(['--------' for _ in range(len(seq_sites))])\n",
        "    except StopIteration: #if file exists, but there are no sites, return default string\n",
        "        return(['--------' for _ in range(len(seq_sites))])\n",
        "\n",
        "def exclude_hits(hits_to_exclude: List[str], all_hits: List[str]) -> List[str]:\n",
        "    nonhit_regions = []\n",
        "    for hit_index, species in enumerate(hits_to_exclude):\n",
        "        nonhit_regions.append(set(all_hits[hit_index]).symmetric_difference(species))\n",
        "    return(nonhit_regions)\n",
        "\n",
        "# pandas .agg func rename\n",
        "def Num_Unique(series: pd.Series) -> int:\n",
        "    return(len(set(series)))\n",
        "\n",
        "def human_hit(series: pd.Series) -> str:\n",
        "    if series.str.contains('hg38').any():\n",
        "        return('Yes')\n",
        "    else:\n",
        "        return('No')\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Takes fimo files and aln files (optional) and generates\n",
        "    summary dataframe containing one motif hit per line\n",
        "    with the following info: \n",
        "    seq ID|AA pos|species hit: seq|min pval|hg38? (yes/no)|hg38 site|hg38 pval|species absent\n",
        "    \"\"\"\n",
        "    t1_start = perf_counter()\n",
        "\n",
        "    s_args = summary_args(\n",
        "        f\"{jobname}/{jobname}.csv\",\n",
        "        custom_FIMO_path,\n",
        "        f\"{custom_seq_path}\",\n",
        "        custom_PSR_path\n",
        "    )\n",
        "\n",
        "    # Check if this file already exists. If so, do nothing.\n",
        "    # if os.path.isfile(s_args.csv_out):\n",
        "    #     sys.exit(f\"File {s_args.csv_out} exists\")\n",
        "    # else:\n",
        "    #     print (f\"Building {s_args.csv_out}...\")\n",
        "\n",
        "    #Set files and columns to extract from\n",
        "    infimo_files = glob_files(s_args.fimo_tsvs.rstrip('/'))\n",
        "    infile_ind = [1, 2, 6, 8] # 'sequence name', 'start', 'p-value', 'matched sequence'\n",
        "    agg_func_text = {'seqIDs': ['first'], # get one representative seqID (first occurrence)\n",
        "                    'start': ['first', 'count'], # get representative start val, and count of species hits\n",
        "                    'species_seqs': [tuple], # summarize species seq hits as tuple\n",
        "                    'matchedseq': [Num_Unique], # num of unique seq hits found\n",
        "                    'species_pvals': [tuple], # scores for each species hit\n",
        "                    'pvalue': 'min', # best hit, no matter what species\n",
        "                    'species': [human_hit]} # Is this a human hit? Yes or No\n",
        "    mlength = 0\n",
        "    for ind, file in enumerate(infimo_files):\n",
        "        # Create dataframe with selected data from fimo file\n",
        "        tsv_data = pd.read_csv(file, sep = '\\t', usecols = infile_ind)\n",
        "        tsv_data = tsv_data.rename({'sequence name': 'seqname', 'start': 'start', 'p-value': 'pvalue', 'matched sequence': 'matchedseq'}, axis=1)\n",
        "        if ind == 0:\n",
        "            mlength = len(tsv_data['matchedseq'].iloc[0])\n",
        "        # Temporary hack, not intended to have .12taxa.fa_\n",
        "        tsv_data[['seqIDs', 'species']] = tsv_data.seqname.str.split('.12taxa.fa_', expand=True)\n",
        "        tsv_data['species_seqs'] = tsv_data['species'].astype(str) + ': ' + tsv_data['matchedseq']\n",
        "        tsv_data['species_pvals'] = tsv_data['species'].astype(str) + ': ' + tsv_data['pvalue'].astype(str)\n",
        "        #Retain unmerged data: hg38 matchedseq and pvalue data\n",
        "        hg_data = tsv_data[tsv_data['species'] == 'hg38'].sort_values('start', axis = 0, ascending = True)\n",
        "        \n",
        "        #collapse tsv_data to one line per motif hit across orgs\n",
        "        tsv_data = (tsv_data.iloc[0: , 0:]\n",
        "                    .groupby(['seqIDs', 'start'], as_index = False)\n",
        "                    .agg(agg_func_text))\n",
        "        \n",
        "        # hard-coded -- this order doesn't need to change\n",
        "        tsv_data.columns = ['sequenceID',\n",
        "                            'start',\n",
        "                            'count',\n",
        "                            'concat_sites',\n",
        "                            'Num_Unique',\n",
        "                            'org_pvals',\n",
        "                            'best_pval',\n",
        "                            'human_hit'] # replace w/ readable colnames \n",
        "\n",
        "        #merge tsv_data to retained hg38 data and export\n",
        "        merged_data = pd.merge(tsv_data, hg_data[['start', 'matchedseq', 'pvalue']],on='start', how='left')\n",
        "        #use seqID, start pt, and species hits to scrape sequences of orgs with no detectable motif\n",
        "        #collect species-relevant motif info\n",
        "        aln_directory = s_args.fasta_dir.rstrip('/')\n",
        "        grp_seqID = tsv_data['sequenceID'].iloc[0]\n",
        "        mstarts = tsv_data.start.astype(int) #motif start sites\n",
        "        sp_hits_to_exclude = tsv_data.concat_sites\n",
        "\n",
        "        #extract protein (AA) seq length, [aln of each motif across all primates] regardless of score\n",
        "        AA_length, sp_mregions = expand_motif_aln(aln_directory, grp_seqID, mstarts, mlength)\n",
        "\n",
        "        #exclude hits already examined\n",
        "        nonhit_mregions = exclude_hits(sp_hits_to_exclude, sp_mregions)\n",
        "        \n",
        "        #create nonhit df to merge\n",
        "        nonhit_df = pd.DataFrame(columns = ['start', 'Non_hits'])\n",
        "        nonhit_df['Non_hits'] = nonhit_mregions\n",
        "        nonhit_df['start'] = tsv_data.start\n",
        "        nonhit_df['AA_seqlength'] = AA_length\n",
        "\n",
        "        #merge nonhit df to merged_data\n",
        "        merged_data = pd.merge(merged_data, nonhit_df[['start', 'Non_hits', 'AA_seqlength']],on='start', how='left')\n",
        "\n",
        "        #OPTIONAL: use seqID and start pt to scrape residues under pos sel (PSRs)\n",
        "        if s_args.PSG_dir != 'skip':\n",
        "            #collect PSG relevant info\n",
        "            PSG_directory = s_args.PSG_dir.rstrip('/')\n",
        "            grp_seqID = tsv_data['sequenceID'][0]\n",
        "            mstarts = tsv_data.start.astype(int) #motif start sites\n",
        "\n",
        "            #extract protein (AA) seq length, [aln of each motif across all primates] regardless of score\n",
        "            PSR_stringmap = map_PSRs(PSG_directory, grp_seqID, mstarts)\n",
        "\n",
        "            #create nonhit df to merge\n",
        "            PSR_df = pd.DataFrame(columns = ['start', 'PSRs'])\n",
        "            PSR_df['start'] = tsv_data.start\n",
        "            PSR_df['FUBAR_PSRs'] = PSR_stringmap\n",
        "\n",
        "            #merge nonhit df to merged_data\n",
        "            merged_data = pd.merge(merged_data, PSR_df[['start', 'FUBAR_PSRs']],on='start', how='left')\n",
        "\n",
        "        #Create csv file if first glob file initiated, otherwise append to existing csv\n",
        "        if ind == 0:\n",
        "            merged_data.rename(columns={'matchedseq': 'human_site', 'pvalue': 'pval_hg38'}, inplace=True)\n",
        "            merged_data.to_csv(s_args.csv_out, index = False, mode = 'w', header=True)\n",
        "        else:\n",
        "            merged_data.to_csv(s_args.csv_out, index = False, mode = 'a', header=False)\n",
        "\n",
        "    t1_stop = perf_counter()\n",
        "    print(\"Elapsed time (seconds):\", t1_stop-t1_start)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "VKUw5aR4vmbh"
      },
      "outputs": [],
      "source": [
        "#@title 5. Merge orthogonal dataset to .csv summary file (<1min)\n",
        "\n",
        "@dataclass\n",
        "class merge_args:\n",
        "    csv_in: str\n",
        "    csv_db: str\n",
        "    csv_out: str\n",
        "    key_column: str\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Appends column-specific data from a db.csv file to a designated .csv file\n",
        "    \"\"\"\n",
        "    m_args=merge_args(\n",
        "        f\"{jobname}/{jobname}.csv\",\n",
        "        prebuiltdb,\n",
        "        f\"{jobname}/annotated_{jobname}.csv\",\n",
        "        \"sequenceID\" # currently hard-coded\n",
        "    )\n",
        "\n",
        "    # Set main input df to merge file\n",
        "    in_df=pd.read_csv(m_args.csv_in)\n",
        "\n",
        "    # Set main db file and columns to merge using config (.ini)\n",
        "    db_df=pd.read_csv(m_args.csv_db)\n",
        "    keycol=m_args.key_column\n",
        "    # Re-order db_df and merge to input df on keycol val\n",
        "    sID_col = db_df.pop(keycol)\n",
        "    db_df.insert(0, sID_col.name, sID_col)\n",
        "    sID, *othercols = db_df.columns\n",
        "    in_df=pd.merge(in_df, db_df[[sID, *othercols]],on=keycol, how='left')\n",
        "\n",
        "    # Hard-coded, this order doesn't need to change\n",
        "    final_cols_order = ['sequenceID', 'Gene_Sym', 'description', 'AA_seqlength', 'start', 'count', 'Num_Unique', 'concat_sites', 'org_pvals', 'Non_hits', 'best_pval', 'human_site', 'pval_hg38', 'FUBAR_PSRs', 'Resource_Plate', 'Resource_Position', 'hORF_Length', 'PC1', 'Omega', 'calc_AF', 'log_calc_AF', 'human_hit', 'Ifn_u2', 'Ifn_u5', 'Ifn_d2', 'Ifn_d5']\n",
        "    in_df=in_df.loc[:, final_cols_order]\n",
        "    in_df.to_csv(m_args.csv_out, index=False, mode='w', header=True)\n",
        "\n",
        "if use_db:\n",
        "    main()\n",
        "else:\n",
        "    concat_csv = f\"{jobname}/{jobname}.csv\"\n",
        "    desired_csv = f\"{jobname}/annotated_{jobname}.csv\"\n",
        "    os.rename(concat_csv, desired_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "A2CoKKAEv76z",
        "outputId": "6bad42cd-1935-4c07-e030-dfb389dc23a1"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_82df5d8f-7f7d-4823-ba0f-72dde9bafd5d\", \"annotated_primate_seqs_0b0c1.csv\", 115119922)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title 6. Package and download results\n",
        "#@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\".\n",
        "\n",
        "files.download(f\"{jobname}/annotated_{jobname}.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SLiME.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
