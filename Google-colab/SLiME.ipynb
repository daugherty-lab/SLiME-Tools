{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlK2Z4raaKDT"
      },
      "source": [
        "**Important notes:** This notebook pipeline only works with protein sequences (no headers) or uploads of zipped fasta files (with headers). To upload zipped fasta files, click the \"File\" icon on the left panel, and click the \"Upload to session storage\" icon. These files will be erased at the end of each session, or after the time limit imposed by Google Colaboratory (~12 hours)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1. Run pipeline:\n",
        "#@markdown 1) First, set inputs (3. Set inputs).<br>\n",
        "#@markdown 1b) If you selected custom motif options, follow the prompt to upload files. <br>\n",
        "#@markdown 2) Next, click and highlight this cell (1. Run pipeline). <br>\n",
        "#@markdown 3) Click \"Runtime\" from the top menu and \"Run all\" <br> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title <- Install dependencies before running the pipeline. (~3-6min)\n",
        "# install conda package manager\n",
        "# For more information: https://docs.conda.io/en/latest/\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -y -c bioconda meme=4.11.2 &>/dev/null #5.3.0\n",
        "!conda install -y -c conda-forge biopython &>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rYkK7QfP5KCg"
      },
      "outputs": [],
      "source": [
        "#@title 2. Set inputs\n",
        "\n",
        "from google.colab import files\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "import glob2\n",
        "import pandas as pd\n",
        "from time import perf_counter\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "def add_hash(x,y):\n",
        "  '''\n",
        "  Give jobname unique identifier\n",
        "  '''\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "# Acquire user input query sequence\n",
        "query_sequence = '' #@param {type:\"string\"}\n",
        "\n",
        "# Regardless of whether you've input a sequence or not\n",
        "# Acquire boolean (checkbox) value for zipped fasta\n",
        "Upload_zipped_fastas = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Acquire user input jobname\n",
        "jobname = 'primate_seqs' #@param {type:\"string\"}\n",
        "\n",
        "# Acquire user selected drop-down motif option\n",
        "motif_options = \"custom\" #@param [\"none\", \"E75\",\"custom\"]\n",
        "#@markdown * Warning: Do not select more than one motif upload.\n",
        "\n",
        "# Acquire user input pval number\n",
        "pval = 0.00581 #@param {type:\"number\"}\n",
        "\n",
        "# Acquire boolean check for pos. sel. residue (PSR) files\n",
        "enable_PSRs = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Acquire boolean check for prebuilt db .csv\n",
        "enable_prebuiltdb = True #@param {type:\"boolean\"}\n",
        "\n",
        "is_input = False\n",
        "query_sequence = \"\".join(re.split('; |, |\\*|\\n|-| ', query_sequence)).upper() # remove whitespaces\n",
        "if query_sequence:\n",
        "  is_input = True\n",
        "  non_AAs = {'O', 'U', 'X', 'B', 'Z', 'J'}\n",
        "  if non_AAs.intersection(query_sequence): \n",
        "    print(\"Warning: Invalid AA(s) in sequence.\")\n",
        "else:\n",
        "  query_sequence = \"empty\"\n",
        "\n",
        "if Upload_zipped_fastas:\n",
        "  seq_zipfiles = glob2.glob(\"*seqs.zip\")\n",
        "  if seq_zipfiles:\n",
        "    is_input = True\n",
        "    print(\"Zipped file(s) found:\", *seq_zipfiles)\n",
        "  else:\n",
        "    print(\"Zipped file(s) not found. Try reuploading with a seqs.zip suffix.\")\n",
        "\n",
        "if is_input:\n",
        "  # Acquire user input jobname and add unique identifier\n",
        "  basejobname = \"\".join(jobname.split()) # remove whitespaces\n",
        "  basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "  jobname = add_hash(basejobname, query_sequence)\n",
        "  while os.path.isfile(f\"{jobname}.csv\"):\n",
        "    jobname = add_hash(basejobname, ''.join(random.sample(query_sequence,len(query_sequence))))\n",
        "  with open(f\"{jobname}.csv\", \"w\") as text_file:\n",
        "    text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "  print(\"Setting up job:\", jobname)\n",
        "\n",
        "  # Placeholder - this will be the final output\n",
        "  os.mkdir(jobname)\n",
        "  # queries_path=f\"{jobname}/{jobname}.csv\"\n",
        "else:\n",
        "  print(\"No input sequences found.\")\n",
        "\n",
        "# Check for motif input\n",
        "if motif_options == \"E75\":\n",
        "  use_motif = True\n",
        "  custom_motif_path = None #\n",
        "elif motif_options == \"custom\":\n",
        "  print(\"Input MEME-formatted motif .txt file:\")\n",
        "  custom_motif_path = f\"{jobname}/motif/\"\n",
        "  os.mkdir(custom_motif_path)\n",
        "  uploaded_motif = files.upload()\n",
        "  use_motif = True\n",
        "  motif = \"\"\n",
        "  for fn in uploaded_motif.keys():\n",
        "    os.rename(fn, f\"{jobname}/motif/{fn}\")\n",
        "    motif = f\"{custom_motif_path}{fn}\"\n",
        "else:\n",
        "  print(\"Please select/upload a motif.\")\n",
        "  custom_motif_path = None\n",
        "  use_motif = False\n",
        "\n",
        "# check for PSR input\n",
        "if enable_PSRs:\n",
        "  PSRs = glob2.glob(\"*posteriors.zip\")\n",
        "  if PSRs:\n",
        "    is_PSRs = True\n",
        "    print(\"Including PSR file(s) to be mapped.\")\n",
        "  else:\n",
        "    enable_PSRs = False\n",
        "    print(\"PSR file(s) not found. Try reuploading with a posteriors.zip suffix.\")\n",
        "\n",
        "# check for prebuilt db .csv input\n",
        "if enable_prebuiltdb and query_sequence != \"empty\":\n",
        "  print(\"Input pre-built db.csv file:\")\n",
        "  custom_prebuiltdb_path = f\"{jobname}/db/\"\n",
        "  os.mkdir(custom_prebuiltdb_path)\n",
        "  uploaded_db = files.upload()\n",
        "  use_db = True\n",
        "  prebuiltdb = \"\"\n",
        "  for fn in uploaded_db.keys():\n",
        "    os.rename(fn, f\"{jobname}/db/{fn}\")\n",
        "    prebuiltdb = f\"{custom_prebuiltdb_path}{fn}\"\n",
        "else:\n",
        "  print(\"Please select/upload a pre-built db.\")\n",
        "  custom_prebuiltdb_path = None\n",
        "  use_db = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m9kPocI-RGB5"
      },
      "outputs": [],
      "source": [
        "#@title 4. Build sequences and related files\n",
        "t1_start = perf_counter()\n",
        "\n",
        "# Make input directory\n",
        "custom_seq_path = f\"{jobname}/seqs\"\n",
        "os.mkdir(custom_seq_path)\n",
        "\n",
        "# Create .fa for input query and send to directory\n",
        "if query_sequence != \"empty\":\n",
        "  with open(f\"{custom_seq_path}/{jobname}.12taxa.fa\", 'w') as inputfa:\n",
        "    inputfa.write(f\">{jobname}.12taxa.fa_QuerySeq\\n\")\n",
        "    inputfa.write(f\"{query_sequence}\\n\")\n",
        "\n",
        "# Unzip very quietly (qq) and send .fa files to directory\n",
        "if Upload_zipped_fastas:\n",
        "  for seq_zip in seq_zipfiles:\n",
        "    !unzip -qq -j $seq_zip -d $custom_seq_path\n",
        "\n",
        "# Unzip very quietly (qq) and send .posterior.csv files to directory\n",
        "if enable_PSRs:\n",
        "  custom_PSR_path = f\"{jobname}/posteriors\"\n",
        "  os.mkdir(custom_PSR_path)\n",
        "  for PSR in PSRs:\n",
        "    !unzip -qq -j $PSR -d $custom_PSR_path\n",
        "else:\n",
        "  custom_PSR_path = \"\"\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time (seconds):\", t1_stop-t1_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5. Experimental - FIMO Search motif against all inputs (~10min)\n",
        "#@markdown parallelized across ~2 processes given by colab\n",
        "t1_start = perf_counter()\n",
        "\n",
        "# Create directory for FIMO output\n",
        "print(f\"Creating FIMO output directory.\")\n",
        "custom_FIMO_path = f\"{jobname}/FIMO_out\"\n",
        "os.mkdir(custom_FIMO_path)\n",
        "print(f\"Writing files to: {custom_FIMO_path}.\")\n",
        "\n",
        "# Create script to run FIMO on every file in specified directory\n",
        "sh = \"\"\"\n",
        "#!/bin/zsh\n",
        "\n",
        "# \"$#\" = value of the total number of command line args passed\n",
        "# As long as \"$#\" is greater than (-gt) 0 args, keep while loop alive\n",
        "while [[ \"$#\" -gt 0 ]]; do\n",
        "    # Check each case (options/flags) until match is found\n",
        "    case $1 in\n",
        "        # get input following arg option, then shift to next str\n",
        "        -i|--inputdir) inputdir=\"$2\"; shift ;;\n",
        "        -m|--motif) motif=\"$2\"; shift ;;\n",
        "        -p|--pval) pval=\"$2\"; shift ;;\n",
        "        -o|--oc) outputdir=\"$2\"; shift ;;\n",
        "        \n",
        "        # if extra, unmatched options show up, exit\n",
        "        # Exit code 0 - Success\n",
        "        # Exit code 1 - General/misc errors, such as \"divide by zero\" and other impermissible operations\n",
        "        *) echo \"Unknown parameter passed: $1\"; exit 1 ;;\n",
        "    \n",
        "    # end case search (case spelled backwards)\n",
        "    esac\n",
        "    shift # to the next str, if any, then loop\n",
        "done\n",
        "N=4\n",
        "(\n",
        "for f in $inputdir/*.fa\n",
        "do\n",
        "    ((i=i%N)); ((i++==0)) && wait\n",
        "    # get basename of filepath\n",
        "    baseFname=$(basename $f .fa)\n",
        "    # FIMO on each file\n",
        "    fimo --oc \"$outputdir\" --verbosity 1 --text --thresh $pval --max-stored-scores 8000000 \"$motif\" \"$f\" > \"$outputdir/$baseFname\"\"_fimo.tsv\" &\n",
        "done\n",
        ")\n",
        "\"\"\"\n",
        "with open('parallel_FIMO.sh', 'w') as file:\n",
        "  file.write(sh)\n",
        "\n",
        "# Run parallel FIMO script\n",
        "print(f\"Running iterative FIMO...\")\n",
        "!bash parallel_FIMO.sh -i $custom_seq_path -m $motif -p $pval -o $custom_FIMO_path &>/dev/null\n",
        "print(f\"iterative_FIMO completed.\")\n",
        "\n",
        "# Delete empty .tsv files created by FIMO\n",
        "print(f\"Cleaning empty FIMO .tsv files...\")\n",
        "!find $custom_FIMO_path -size 0 -print -delete &>/dev/null\n",
        "print(f\"Directory cleaned.\")\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time (seconds):\", t1_stop-t1_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DovTgvedFDF4"
      },
      "outputs": [],
      "source": [
        "#@title 5. FIMO Search motif against all inputs (~10min)\n",
        "#@markdown this is too slow -- parallelize it\n",
        "\n",
        "# Create directory for FIMO output\n",
        "print(f\"Creating FIMO output directory.\")\n",
        "custom_FIMO_path = f\"{jobname}/FIMO_out\"\n",
        "os.mkdir(custom_FIMO_path)\n",
        "print(f\"Writing files to: {custom_FIMO_path}.\")\n",
        "\n",
        "# Run FIMO on every file in specified directory\n",
        "print(f\"Running iterative FIMO...\")\n",
        "for in_fa in glob2.glob(f\"{custom_seq_path}/*.fa\"):\n",
        "  # get basename of filepath\n",
        "  baseFname = in_fa.split(\".\")[0].split(\"/\")[-1]\n",
        "  # FIMO on each file\n",
        "  !fimo --oc $custom_FIMO_path --verbosity 1 --text --thresh $pval --max-stored-scores 8000000 \\\n",
        "  $motif $in_fa > \"$custom_FIMO_path/$baseFname\"\"_fimo.tsv\" 2> /dev/null\n",
        "print(f\"iterative_FIMO completed.\")\n",
        "\n",
        "# Delete empty .tsv files created by FIMO\n",
        "print(f\"Cleaning empty FIMO .tsv files...\")\n",
        "!find $custom_FIMO_path -size 0 -print -delete\n",
        "print(f\"Directory cleaned.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cum25OU5XgTM"
      },
      "outputs": [],
      "source": [
        "#@title 6. Summarize all FIMO .tsv inputs (~15min)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class arg_collection:\n",
        "    csv_out: str\n",
        "    fimo_tsvs: List[str]\n",
        "    fasta_dir: str\n",
        "    PSG_dir: str\n",
        "    def __post_init__(self):\n",
        "        if not self.PSG_dir:\n",
        "            self.PSG_dir = 'skip'\n",
        "\n",
        "def glob_files(path: str) -> List[str]:\n",
        "    return(glob2.glob(f'{path}/*.tsv'))\n",
        "\n",
        "def expand_motif_aln(aln_dir: str, in_seqID: str, seq_sites: List[int]) -> (int, List[str]):\n",
        "    '''Default shows only the species sequences with motif hits.\n",
        "    When an aln dir is specified, this function is invoked to\n",
        "    show the motif alignment across both hits and nonhits.'''\n",
        "    from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "\n",
        "    # collect species name from title and relevant motif alignment from sequence\n",
        "    with open(f'{aln_dir}/{in_seqID}.12taxa.fa') as aln_file:\n",
        "        species_regions = []\n",
        "        seq_length = 0\n",
        "        for title, sequence in SimpleFastaParser(aln_file):\n",
        "            species_name = title.split('_')[-1]\n",
        "            if species_name == 'hg38' or species_name == 'QuerySeq':\n",
        "                seq_length = len(sequence.replace('-',''))\n",
        "            species_regions.append([f'{species_name}: {sequence[pos-1:pos+7]}' for pos in seq_sites])\n",
        "    return(seq_length, list(map(list, zip(*species_regions))))\n",
        "\n",
        "def map_PSRs(PSG_dir: str, in_seqID: str, seq_sites: List[int]) -> List[List[str]]:\n",
        "    '''Returns stringmap of Positive Selection at Residues (PSRs) from dir of FUBAR files, \n",
        "    if relevant to the motif range (pos-1:pos+7). PSRs are recorded as '+', \n",
        "    and non-PSRs are recorded as '-'.'''\n",
        "    site_map = [list(range(pos-1,pos+7)) for pos in seq_sites]\n",
        "    PSRs = []\n",
        "    try: # File may or may not exist, but if it does, collect PSR entries\n",
        "        with open(f'{PSG_dir}/{in_seqID}.12taxa.fa.12taxon.tree.grid_info.posteriors.csv') as PSG_file:\n",
        "            next(PSG_file) # Skip first line\n",
        "            for line in PSG_file:\n",
        "                PSR = int(line.split('0.')[0])\n",
        "                PSRs.append(PSR)\n",
        "        \n",
        "        # map sites with presence '+' or absence '_' of PSR\n",
        "        for site_i, site in enumerate(site_map):\n",
        "            for pos_i, pos in enumerate(site):\n",
        "                if pos in PSRs:\n",
        "                    site_map[site_i][pos_i] = '+'\n",
        "                else:\n",
        "                    site_map[site_i][pos_i] = '-'\n",
        "            site_map[site_i] = ''.join(site)\n",
        "        return(site_map)\n",
        "    except IOError: #if file doesn't exist, return default string\n",
        "        return(['--------' for _ in range(len(seq_sites))])\n",
        "    except StopIteration: #if file exists, but there are no sites, return default string\n",
        "        return(['--------' for _ in range(len(seq_sites))])\n",
        "\n",
        "def exclude_hits(hits_to_exclude: List[str], all_hits: List[str]) -> List[str]:\n",
        "    nonhit_regions = []\n",
        "    for hit_index, species in enumerate(hits_to_exclude):\n",
        "        nonhit_regions.append(set(all_hits[hit_index]).symmetric_difference(species))\n",
        "    return(nonhit_regions)\n",
        "\n",
        "# pandas .agg func rename\n",
        "def Num_Unique(series: pd.Series) -> int:\n",
        "    return(len(set(series)))\n",
        "\n",
        "def human_hit(series: pd.Series) -> str:\n",
        "    if series.str.contains('hg38').any():\n",
        "        return('Yes')\n",
        "    else:\n",
        "        return('No')\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Takes fimo files and aln files (optional) and generates\n",
        "    summary dataframe containing one motif hit per line\n",
        "    with the following info: \n",
        "    seq ID|AA pos|species hit: seq|min pval|hg38? (yes/no)|hg38 site|hg38 pval|species absent\n",
        "    \"\"\"\n",
        "    t1_start = perf_counter()\n",
        "\n",
        "    args = arg_collection(\n",
        "        f\"{jobname}/{jobname}.csv\",\n",
        "        custom_FIMO_path,\n",
        "        f\"{custom_seq_path}\",\n",
        "        custom_PSR_path\n",
        "    )\n",
        "\n",
        "    # Check if this file already exists. If so, do nothing.\n",
        "    if os.path.isfile(args.csv_out):\n",
        "        sys.exit(f\"File {args.csv_out} exists\")\n",
        "    else:\n",
        "        print (f\"Building {args.csv_out}...\")\n",
        "\n",
        "    #Set files and columns to extract from\n",
        "    infimo_files = glob_files(args.fimo_tsvs.rstrip('/'))\n",
        "    infile_ind = [1, 2, 6, 8] # 'sequence name', 'start', 'p-value', 'matched sequence'\n",
        "    agg_func_text = {'seqIDs': ['first'], # get one representative seqID (first occurrence)\n",
        "                    'start': ['first', 'count'], # get representative start val, and count of species hits\n",
        "                    'species_seqs': [tuple], # summarize species seq hits as tuple\n",
        "                    'matchedseq': [Num_Unique], # num of unique seq hits found\n",
        "                    'species_pvals': [tuple], # scores for each species hit\n",
        "                    'pvalue': 'min', # best hit, no matter what species\n",
        "                    'species': [human_hit]} # Is this a human hit? Yes or No\n",
        "\n",
        "    for ind, file in enumerate(infimo_files):\n",
        "        # Create dataframe with selected data from fimo file\n",
        "        tsv_data = pd.read_csv(file, sep = '\\t', usecols = infile_ind, \n",
        "                                names = ['seqname', 'start', 'pvalue', 'matchedseq'])\n",
        "        \n",
        "        # Temporary hack, not intended to have .12taxa.fa_\n",
        "        tsv_data[['seqIDs', 'species']] = tsv_data.seqname.str.split('.12taxa.fa_', expand=True)\n",
        "        tsv_data['species_seqs'] = tsv_data['species'].astype(str) + ': ' + tsv_data['matchedseq']\n",
        "        tsv_data['species_pvals'] = tsv_data['species'].astype(str) + ': ' + tsv_data['pvalue'].astype(str)\n",
        "\n",
        "        #Retain unmerged data: hg38 matchedseq and pvalue data\n",
        "        hg_data = tsv_data[tsv_data['species'] == 'hg38'].sort_values('start', axis = 0, ascending = True)\n",
        "\n",
        "        #collapse tsv_data to one line per motif hit across orgs\n",
        "        tsv_data = (tsv_data.iloc[1: , 1:]\n",
        "                    .groupby(['seqIDs', 'start'], as_index = False)\n",
        "                    .agg(agg_func_text))\n",
        "\n",
        "        # hard-coded -- this order doesn't need to change\n",
        "        tsv_data.columns = ['sequenceID', 'start', 'count', 'concat_sites', 'Num_Unique', 'org_pvals', 'best_pval', 'human_hit'] # replace w/ readable colnames \n",
        "\n",
        "        #merge tsv_data to retained hg38 data and export\n",
        "        merged_data = pd.merge(tsv_data, hg_data[['start', 'matchedseq', 'pvalue']],on='start', how='left')\n",
        "\n",
        "        #use seqID, start pt, and species hits to scrape sequences of orgs with no detectable motif\n",
        "        #collect species-relevant motif info\n",
        "        aln_directory = args.fasta_dir.rstrip('/')\n",
        "        grp_seqID = tsv_data['sequenceID'][0]\n",
        "        mstarts = tsv_data.start.astype(int) #motif start sites\n",
        "        sp_hits_to_exclude = tsv_data.concat_sites\n",
        "\n",
        "        #extract protein (AA) seq length, [aln of each motif across all primates] regardless of score\n",
        "        AA_length, sp_mregions = expand_motif_aln(aln_directory, grp_seqID, mstarts)\n",
        "\n",
        "        #exclude hits already examined\n",
        "        nonhit_mregions = exclude_hits(sp_hits_to_exclude, sp_mregions)\n",
        "        \n",
        "        #create nonhit df to merge\n",
        "        nonhit_df = pd.DataFrame(columns = ['start', 'Non_hits'])\n",
        "        nonhit_df['Non_hits'] = nonhit_mregions\n",
        "        nonhit_df['start'] = tsv_data.start\n",
        "        nonhit_df['AA_seqlength'] = AA_length\n",
        "\n",
        "        #merge nonhit df to merged_data\n",
        "        merged_data = pd.merge(merged_data, nonhit_df[['start', 'Non_hits', 'AA_seqlength']],on='start', how='left')\n",
        "\n",
        "        #OPTIONAL: use seqID and start pt to scrape residues under pos sel (PSRs)\n",
        "        if args.PSG_dir != 'skip':\n",
        "            #collect PSG relevant info\n",
        "            PSG_directory = args.PSG_dir.rstrip('/')\n",
        "            grp_seqID = tsv_data['sequenceID'][0]\n",
        "            mstarts = tsv_data.start.astype(int) #motif start sites\n",
        "\n",
        "            #extract protein (AA) seq length, [aln of each motif across all primates] regardless of score\n",
        "            PSR_stringmap = map_PSRs(PSG_directory, grp_seqID, mstarts)\n",
        "\n",
        "            #create nonhit df to merge\n",
        "            PSR_df = pd.DataFrame(columns = ['start', 'PSRs'])\n",
        "            PSR_df['start'] = tsv_data.start\n",
        "            PSR_df['FUBAR_PSRs'] = PSR_stringmap\n",
        "\n",
        "            #merge nonhit df to merged_data\n",
        "            merged_data = pd.merge(merged_data, PSR_df[['start', 'FUBAR_PSRs']],on='start', how='left')\n",
        "\n",
        "        #Create csv file if first glob file initiated, otherwise append to existing csv\n",
        "        if ind == 0:\n",
        "            merged_data.rename(columns={'matchedseq': 'human_site', 'pvalue': 'pval_hg38'}, inplace=True)\n",
        "            merged_data.to_csv(args.csv_out, index = False, mode = 'w', header=True)\n",
        "        else:\n",
        "            merged_data.to_csv(args.csv_out, index = False, mode = 'a', header=False)\n",
        "\n",
        "    t1_stop = perf_counter()\n",
        "    print(\"Elapsed time (seconds):\", t1_stop-t1_start)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lD8JIezmwHrQ"
      },
      "outputs": [],
      "source": [
        "#@title Create orthogonal dataset .csv (<1min)\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def parse_args():\n",
        "    parser=argparse.ArgumentParser(prog='merge-dbs.py', conflict_handler='resolve')\n",
        "    parser.add_argument('-db', type=str, required=True, help='=> path/to/main_db.csv')\n",
        "    parser.add_argument('-db_dir', type=str, required=True, help='=> path/to/database_directory')\n",
        "    parser.add_argument('-o', type=str, required=True, help='=> path/to/merged_outfile.csv')\n",
        "    return(parser.parse_args())\n",
        "\n",
        "def glob_files(path: str) -> list[str]:\n",
        "    return(glob.glob(f'{path}/*'))\n",
        "\n",
        "def auto_merge(in_df: pd.DataFrame, glob_files: list[str]) -> tuple[pd.DataFrame, list[str]]:\n",
        "    leftovers=[]\n",
        "    for file in glob_files:\n",
        "        # Create dataframe with selected data from each compatible db files in loop\n",
        "        filename=file.split('/')[-1]\n",
        "        db=pd.read_csv(file)\n",
        "        cols=list(db)\n",
        "        col_intersect=list(in_df.columns.intersection(db.columns)) # get key col, assume one\n",
        "\n",
        "        # If shared key col exists, unpack to unique var, else skip/report file error\n",
        "        if col_intersect:\n",
        "            cols.insert(0, cols.pop(cols.index(col_intersect[0]))) # move intersected key col to the front\n",
        "            db=db.loc[:, cols]\n",
        "            sharedID, *othercols=db.columns\n",
        "            print(f'{filename}: joined at {sharedID}')\n",
        "        else:\n",
        "            print(f'{filename}: Missing shared key column')\n",
        "            leftovers.append(file)\n",
        "            continue\n",
        "        # Merge cleaned up dfs\n",
        "        db.drop_duplicates(subset=sharedID, keep='first', inplace=True)\n",
        "        in_df=pd.merge(in_df, db[[sharedID, *othercols]],on=sharedID, how='left')\n",
        "    return(in_df, leftovers)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Appends column-specific data from other .csv files to a designated .csv file\n",
        "    \"\"\"\n",
        "    args=parse_args()\n",
        "\n",
        "    # Set main db file\n",
        "    db_csv=pd.read_csv(args.db)\n",
        "\n",
        "    # Set directory of db files to merge\n",
        "    db_files=glob_files(args.db_dir.rstrip('/'))\n",
        "\n",
        "    # Merge each db_file to in_csv by shared key col, record unmerged files\n",
        "    db_csv, db_leftovers=auto_merge(db_csv, db_files)\n",
        "    \n",
        "    # Retry with unmerged files, if applicable, then export .csv\n",
        "    if db_leftovers:\n",
        "        db_csv, remaining_leftovers=auto_merge(db_csv, db_leftovers)\n",
        "        db_csv.to_csv(args.o, index=False, mode='w', header=True)\n",
        "        if remaining_leftovers:\n",
        "            remaining_filenames=[file.split('/')[-1] for file in remaining_leftovers]\n",
        "            print(f'Remaining un-merged files: {remaining_filenames}')\n",
        "    else:\n",
        "        db_csv.to_csv(args.o, index=False, mode='w', header=True)\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VKUw5aR4vmbh"
      },
      "outputs": [],
      "source": [
        "#@title Merge orthogonal dataset to .csv summary file (<1min)\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "def parse_args():\n",
        "    parser=argparse.ArgumentParser(prog='merge-to-db.py', conflict_handler='resolve')\n",
        "    parser.add_argument('-i', type=str, required=True, help='=> path/to/infile.csv')\n",
        "    parser.add_argument('-db', type=str, required=True, help='=> path/to/main_db')\n",
        "    parser.add_argument('-keycolumn', type=str, required=True, help='=> key column to merge on')\n",
        "    parser.add_argument('-o', type=str, required=True, help='=> path/to/merged_outfile.csv')\n",
        "    return(parser.parse_args())\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Appends column-specific data from a db.csv file to a designated .csv file\n",
        "    \"\"\"\n",
        "    args=parse_args()\n",
        "\n",
        "    # Set main input df to merge file\n",
        "    in_df=pd.read_csv(args.i)\n",
        "\n",
        "    # Set main db file and columns to merge using config (.ini)\n",
        "    db_df=pd.read_csv(args.db)\n",
        "    keycol=args.keycolumn\n",
        "    # Re-order db_df and merge to input df on keycol val\n",
        "    sID_col = db_df.pop(keycol)\n",
        "    db_df.insert(0, sID_col.name, sID_col)\n",
        "    sID, *othercols = db_df.columns\n",
        "    in_df=pd.merge(in_df, db_df[[sID, *othercols]],on=keycol, how='left')\n",
        "\n",
        "    # Hard-coded, this order doesn't need to change\n",
        "    final_cols_order = ['sequenceID', 'Gene_Sym', 'description', 'AA_seqlength', \n",
        "        'start', 'count', 'Num_Unique', 'concat_sites', 'org_pvals', 'Non_hits', \n",
        "        'best_pval', 'human_site', 'pval_hg38', 'FUBAR_PSRs', 'Resource_Plate', \n",
        "        'Resource_Position', 'hORF_Length', 'PC1', 'Omega', 'calc_AF', 'log_calc_AF', \n",
        "        'human_hit', 'Ifn_u2', 'Ifn_u5', 'Ifn_d2', 'Ifn_d5']\n",
        "    in_df=in_df.loc[:, final_cols_order]\n",
        "    in_df.to_csv(args.o, index=False, mode='w', header=True)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A2CoKKAEv76z"
      },
      "outputs": [],
      "source": [
        "#@title Package and download results\n",
        "#@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "# if msa_mode == \"custom\":\n",
        "#   print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "\n",
        "# !zip -FSr $jobname\".result.zip\" config.json $jobname*\".json\" $jobname*\".a3m\" $jobname*\"relaxed_rank_\"*\".pdb\" \"cite.bibtex\" $jobname*\".png\"\n",
        "# files.download(f\"{jobname}.result.zip\")\n",
        "\n",
        "# if save_to_google_drive == True and drive:\n",
        "#   uploaded = drive.CreateFile({'title': f\"{jobname}.result.zip\"})\n",
        "#   uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
        "#   uploaded.Upload()\n",
        "#   print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SLiME.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
