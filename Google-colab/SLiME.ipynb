{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlK2Z4raaKDT"
      },
      "source": [
        "**Important notes:** This notebook pipeline only works with protein sequences (no headers) or uploads of zipped fasta files (with headers). To upload zipped fasta files, click the \"File\" icon on the left panel, and click the \"Upload to session storage\" icon. These files will be erased at the end of each session, or after the time limit imposed by Google Colaboratory (~12 hours)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rYkK7QfP5KCg"
      },
      "outputs": [],
      "source": [
        "#@title Set inputs\n",
        "\n",
        "# Adapted from AlphaFold2\n",
        "from google.colab import files\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "import glob2\n",
        "\n",
        "def add_hash(x,y):\n",
        "  '''\n",
        "  Give jobname unique identifier\n",
        "  '''\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "# Acquire user input query sequence\n",
        "# Need to add test to force amino acid letters and - only\n",
        "\n",
        "non_AAs = {'O', 'U', 'X', 'B', 'Z', 'J'}\n",
        "is_input = False\n",
        "query_sequence = 'BANANA' #@param {type:\"string\"}\n",
        "query_sequence = \"\".join(re.split('; |, |\\*|\\n|-| ', query_sequence)).upper() # remove whitespaces\n",
        "if query_sequence:\n",
        "  is_input = True\n",
        "  if non_AAs.intersection(query_sequence): \n",
        "    print(\"Warning: Invalid AA(s) in sequence.\") \n",
        "else:\n",
        "  query_sequence = \"empty\"\n",
        "\n",
        "# Regardless of whether you've input a sequence or not\n",
        "# Override if true\n",
        "Upload_zipped_fastas = True #@param {type:\"boolean\"}\n",
        "if Upload_zipped_fastas:\n",
        "  zipfiles = glob2.glob(\"*.zip\")\n",
        "  if zipfiles:\n",
        "    is_input = True\n",
        "    print(\"Zipped file(s) found:\", *zipfiles)\n",
        "    print(\"Unzipping file(s)\")\n",
        "  else:\n",
        "    print(\"Zipped file(s) not found.\")\n",
        "\n",
        "if is_input:\n",
        "  # Acquire user input jobname and add unique identifier\n",
        "  jobname = 'Test' #@param {type:\"string\"}\n",
        "  basejobname = \"\".join(jobname.split()) # remove whitespaces\n",
        "  basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "  jobname = add_hash(basejobname, query_sequence)\n",
        "  while os.path.isfile(f\"{jobname}.csv\"):\n",
        "    jobname = add_hash(basejobname, ''.join(random.sample(query_sequence,len(query_sequence))))\n",
        "  with open(f\"{jobname}.csv\", \"w\") as text_file:\n",
        "      text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "  print(\"Setting up job:\", jobname)\n",
        "\n",
        "  # Placeholder - this will be the final output\n",
        "  os.mkdir(jobname)\n",
        "  # queries_path=f\"{jobname}/{jobname}.csv\"\n",
        "\n",
        "  motif_options = \"custom\" #@param [\"none\", \"E75\",\"custom\"]\n",
        "  #@markdown * Warning: Do not select more than one motif upload.\n",
        "  if motif_options == \"E75\":\n",
        "    use_motif = True\n",
        "    custom_motif_path = None #\n",
        "  elif motif_options == \"custom\":\n",
        "    print(\"Input MEME-formatted motif .txt file:\")\n",
        "    custom_motif_path = f\"{jobname}/motif/\"\n",
        "    os.mkdir(custom_motif_path)\n",
        "    uploaded_motif = files.upload()\n",
        "    use_motif = True\n",
        "    for fn in uploaded_motif.keys():\n",
        "      os.rename(fn, f\"{jobname}/motif/{fn}\")\n",
        "  else:\n",
        "    custom_motif_path = None\n",
        "    use_motif = False\n",
        "else:\n",
        "  print(\"No inputs found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8kMUP7vluLm-"
      },
      "outputs": [],
      "source": [
        "#@title <- Install dependencies (~6min)\n",
        "from time import perf_counter\n",
        "\n",
        "t1_start = perf_counter()\n",
        "# install conda package manager\n",
        "# For more information: https://docs.conda.io/en/latest/\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -c bioconda meme=5.3.0\n",
        "# !conda install -c conda-forge biopython\n",
        "# import Bio\n",
        "import pandas as pd\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time (seconds):\", t1_stop-t1_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m9kPocI-RGB5"
      },
      "outputs": [],
      "source": [
        "#@title <- Build sequences\n",
        "t1_start = perf_counter()\n",
        "\n",
        "# Make input directory\n",
        "custom_seq_path = f\"{jobname}/seqs\"\n",
        "os.mkdir(custom_seq_path)\n",
        "\n",
        "# Create .fa for input query and send to directory\n",
        "if query_sequence:\n",
        "  with open(f\"{custom_seq_path}/{jobname}.fa\", 'w') as inputfa:\n",
        "    inputfa.write(f\">{jobname}\\n\")\n",
        "    inputfa.write(f\"{query_sequence}\\n\")\n",
        "\n",
        "# Unzip and send .fa files to directory\n",
        "if zipfiles:\n",
        "  for zip in zipfiles:\n",
        "    # very quietly (qq) unzip the file(s)\n",
        "    !unzip $zip -qq -d $custom_seq_path\n",
        "\n",
        "# Create directory for FIMO output (next step)\n",
        "custom_FIMO_path = f\"{jobname}/FIMO_out\"\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time (seconds):\", t1_stop-t1_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DovTgvedFDF4"
      },
      "outputs": [],
      "source": [
        "#@title FIMO Search motif against all inputs (~10min)\n",
        "#@markdown * Still need pval and motif\n",
        "#@markdown * This needs to be written in Python with os\n",
        "%%bash\n",
        "\n",
        "# Exit immediately if exit status is non-zero (failed operation)\n",
        "set -e\n",
        "\n",
        "# Create FIMO output directory\n",
        "echo \"Creating directory: $custom_FIMO_path.\"\n",
        "mkdir $custom_FIMO_path\n",
        "echo \"Writing files to: $custom_FIMO_path.\"\n",
        "\n",
        "# Run FIMO on every file in specified directory\n",
        "echo \"Running recursive_FIMO...\"\n",
        "for f in $custom_seq_path/*.fa; do\n",
        "  echo \"Processing $f file...\"\n",
        "  # get basename of filepath\n",
        "  baseFname=$(basename $f .fa)\n",
        "  # FIMO on each file\n",
        "  fimo --oc $custom_FIMO_path --verbosity 1 --text --thresh $pval --max-stored-scores 8000000 \\\n",
        "  $motif $f > \"$custom_FIMO_path/$baseFname\"\"_fimo.tsv\" 2> /dev/null\n",
        "done\n",
        "echo \"recursive_FIMO completed.\"\n",
        "\n",
        "# Delete empty .tsv files created by FIMO\n",
        "echo \"Cleaning empty FIMO .tsv files...\"\n",
        "find $custom_FIMO_path -size 0 -print -delete\n",
        "echo \"Directory cleaned.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cum25OU5XgTM"
      },
      "outputs": [],
      "source": [
        "#@title Summarize all FIMO .tsv inputs (~15min)\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import sys\n",
        "from time import perf_counter\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(prog = 'concat-hitsum.py', conflict_handler = 'resolve')\n",
        "    # parser.add_argument('-bl', type = str, required = True, help = '=> .txt with organism blacklist e.g. mm10')\n",
        "    parser.add_argument('-fimodir', type = str, required = True, help = '=> path/to/fimo_directory')\n",
        "    parser.add_argument('-alndir', type = str, required = False, \n",
        "                        help = '=> path/to/alignments_directory. ONLY if you want to list orgs w/o motif hits')\n",
        "    parser.add_argument('-PSGdir', type = str, required = False, \n",
        "                        help = '=> path/to/directory_with_FUBAR_outfiles. ONLY if you want to position-specific +ve info')\n",
        "    parser.add_argument('-o', type = str, required = True, help = '=> path/to/outfile.csv')\n",
        "    return(parser.parse_args())\n",
        "\n",
        "def glob_files(path: str) -> list[str]:\n",
        "    return(glob.glob(f'{path}/*.tsv'))\n",
        "\n",
        "def expand_motif_aln(aln_dir: str, in_seqID: str, seq_sites: list[int]) -> (int, list[str]):\n",
        "    '''Default shows only the species sequences with motif hits.\n",
        "    When an aln dir is specified, this function is invoked to\n",
        "    show the motif alignment across both hits and nonhits.'''\n",
        "    from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "\n",
        "    # collect species name from title and relevant motif alignment from sequence\n",
        "    with open(f'{aln_dir}/{in_seqID}.12taxa.fa') as aln_file:\n",
        "        species_regions = []\n",
        "        seq_length = 0\n",
        "        for title, sequence in SimpleFastaParser(aln_file):\n",
        "            species_name = title.split('_')[-1]\n",
        "            if species_name == 'hg38':\n",
        "                seq_length = len(sequence.replace('-',''))\n",
        "            species_regions.append([f'{species_name}: {sequence[pos-1:pos+7]}' for pos in seq_sites])\n",
        "    return(seq_length, list(map(list, zip(*species_regions))))\n",
        "\n",
        "def map_PSRs(PSG_dir: str, in_seqID: str, seq_sites: list[int]) -> list[list[str]]:\n",
        "    '''Returns stringmap of Positive Selection at Residues (PSRs) from dir of FUBAR files, \n",
        "    if relevant to the motif range (pos-1:pos+7). PSRs are recorded as '+', \n",
        "    and non-PSRs are recorded as '-'.'''\n",
        "    site_map = [list(range(pos-1,pos+7)) for pos in seq_sites]\n",
        "    PSRs = []\n",
        "    try: # File may or may not exist, but if it does, collect PSR entries\n",
        "        with open(f'{PSG_dir}/{in_seqID}.12taxa.fa.12taxon.tree.grid_info.posteriors.csv') as PSG_file:\n",
        "            next(PSG_file) # Skip first line\n",
        "            for line in PSG_file:\n",
        "                PSR = int(line.split('0.')[0])\n",
        "                PSRs.append(PSR)\n",
        "        \n",
        "        # map sites with presence '+' or absence '_' of PSR\n",
        "        for site_i, site in enumerate(site_map):\n",
        "            for pos_i, pos in enumerate(site):\n",
        "                if pos in PSRs:\n",
        "                    site_map[site_i][pos_i] = '+'\n",
        "                else:\n",
        "                    site_map[site_i][pos_i] = '-'\n",
        "            site_map[site_i] = ''.join(site)\n",
        "        return(site_map)\n",
        "    except IOError: #if file doesn't exist, return default string\n",
        "        return(['--------' for _ in range(len(seq_sites))])\n",
        "    except StopIteration: #if file exists, but there are no sites, return default string\n",
        "        return(['--------' for _ in range(len(seq_sites))])\n",
        "\n",
        "def exclude_hits(hits_to_exclude: list[str], all_hits: list[str]) -> list[str]:\n",
        "    nonhit_regions = []\n",
        "    for hit_index, species in enumerate(hits_to_exclude):\n",
        "        nonhit_regions.append(set(all_hits[hit_index]).symmetric_difference(species))\n",
        "    return(nonhit_regions)\n",
        "\n",
        "# pandas .agg func rename\n",
        "def Num_Unique(series: pd.Series) -> int:\n",
        "    return(len(set(series)))\n",
        "\n",
        "def human_hit(series: pd.Series) -> str:\n",
        "    if series.str.contains('hg38').any():\n",
        "        return('Yes')\n",
        "    else:\n",
        "        return('No')\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Takes fimo files and aln files (optional) and generates\n",
        "    summary dataframe containing one motif hit per line\n",
        "    with the following info: \n",
        "    seq ID|AA pos|species hit: seq|min pval|hg38? (yes/no)|hg38 site|hg38 pval|species absent\n",
        "    \"\"\"\n",
        "    t1_start = perf_counter()\n",
        "\n",
        "    args = parse_args()\n",
        "\n",
        "    if os.path.isfile(args.o):\n",
        "        sys.exit(f\"File {args.o} exists\")\n",
        "    else:\n",
        "        print (f\"Building {args.o}...\")\n",
        "\n",
        "    #Set files and columns to extract from\n",
        "    infimo_files = glob_files(args.fimodir.rstrip('/'))\n",
        "    infile_ind = [1, 2, 6, 8] # 'sequence name', 'start', 'p-value', 'matched sequence'\n",
        "\n",
        "    agg_func_text = {'seqIDs': ['first'], # get one representative seqID (first occurrence)\n",
        "                    'start': ['first', 'count'], # get representative start val, and count of species hits\n",
        "                    'species_seqs': [tuple], # summarize species seq hits as tuple\n",
        "                    'matchedseq': [Num_Unique], # num of unique seq hits found\n",
        "                    'species_pvals': [tuple], # scores for each species hit\n",
        "                    'pvalue': 'min', # best hit, no matter what species\n",
        "                    'species': [human_hit]} # Is this a human hit? Yes or No\n",
        "\n",
        "    for ind, file in enumerate(infimo_files):\n",
        "\n",
        "        #Create dataframe with selected data from fimo file\n",
        "        tsv_data = pd.read_csv(file, sep = '\\t', usecols = infile_ind, \n",
        "                                names = ['seqname', 'start', 'pvalue', 'matchedseq'])\n",
        "        tsv_data[['seqIDs', 'species']] = tsv_data.seqname.str.split('.12taxa.fa_', expand=True)\n",
        "        tsv_data['species_seqs'] = tsv_data['species'].astype(str) + ': ' + tsv_data['matchedseq']\n",
        "        tsv_data['species_pvals'] = tsv_data['species'].astype(str) + ': ' + tsv_data['pvalue'].astype(str)\n",
        "\n",
        "        #Retain unmerged data: hg38 matchedseq and pvalue data\n",
        "        hg_data = tsv_data[tsv_data['species'] == 'hg38'].sort_values('start', axis = 0, ascending = True)\n",
        "\n",
        "        #collapse tsv_data to one line per motif hit across orgs\n",
        "        tsv_data = (tsv_data.iloc[1: , 1:]\n",
        "                    .groupby(['seqIDs', 'start'], as_index = False)\n",
        "                    .agg(agg_func_text))\n",
        "        \n",
        "        # hard-coded -- this order doesn't need to change\n",
        "        tsv_data.columns = ['sequenceID', 'start', 'count', 'concat_sites', 'Num_Unique', 'org_pvals', 'best_pval', 'human_hit'] # replace w/ readable colnames \n",
        "\n",
        "        #merge tsv_data to retained hg38 data and export\n",
        "        merged_data = pd.merge(tsv_data, hg_data[['start', 'matchedseq', 'pvalue']],on='start', how='left')\n",
        "\n",
        "        #OPTIONAL: use seqID, start pt, and species hits to scrape sequences of orgs with no detectable motif\n",
        "        if args.alndir:\n",
        "            #collect species-relevant motif info\n",
        "            aln_directory = args.alndir.rstrip('/')\n",
        "            grp_seqID = tsv_data['sequenceID'][0]\n",
        "            mstarts = tsv_data.start.astype(int) #motif start sites\n",
        "            sp_hits_to_exclude = tsv_data.concat_sites\n",
        "\n",
        "            #extract protein (AA) seq length, [aln of each motif across all primates] regardless of score\n",
        "            AA_length, sp_mregions = expand_motif_aln(aln_directory, grp_seqID, mstarts)\n",
        "\n",
        "            #exclude hits already examined\n",
        "            nonhit_mregions = exclude_hits(sp_hits_to_exclude, sp_mregions)\n",
        "            \n",
        "            #create nonhit df to merge\n",
        "            nonhit_df = pd.DataFrame(columns = ['start', 'Non_hits'])\n",
        "            nonhit_df['Non_hits'] = nonhit_mregions\n",
        "            nonhit_df['start'] = tsv_data.start\n",
        "            nonhit_df['AA_seqlength'] = AA_length\n",
        "\n",
        "            #merge nonhit df to merged_data\n",
        "            merged_data = pd.merge(merged_data, nonhit_df[['start', 'Non_hits', 'AA_seqlength']],on='start', how='left')\n",
        "\n",
        "        #OPTIONAL: use seqID and start pt to scrape residues under pos sel (PSRs)\n",
        "        if args.PSGdir:\n",
        "            #collect PSG relevant info\n",
        "            PSG_directory = args.PSGdir.rstrip('/')\n",
        "            grp_seqID = tsv_data['sequenceID'][0]\n",
        "            mstarts = tsv_data.start.astype(int) #motif start sites\n",
        "\n",
        "            #extract protein (AA) seq length, [aln of each motif across all primates] regardless of score\n",
        "            PSR_stringmap = map_PSRs(PSG_directory, grp_seqID, mstarts)\n",
        "\n",
        "            #create nonhit df to merge\n",
        "            PSR_df = pd.DataFrame(columns = ['start', 'PSRs'])\n",
        "            PSR_df['start'] = tsv_data.start\n",
        "            PSR_df['FUBAR_PSRs'] = PSR_stringmap\n",
        "\n",
        "            #merge nonhit df to merged_data\n",
        "            merged_data = pd.merge(merged_data, PSR_df[['start', 'FUBAR_PSRs']],on='start', how='left')\n",
        "\n",
        "        #Create csv file if first glob file initiated, otherwise append to existing csv\n",
        "        if ind == 0:\n",
        "            merged_data.rename(columns={'matchedseq': 'human_site', 'pvalue': 'pval_hg38'}, inplace=True)\n",
        "            merged_data.to_csv(args.o, index = False, mode = 'w', header=True)\n",
        "        else:\n",
        "            merged_data.to_csv(args.o, index = False, mode = 'a', header=False)\n",
        "\n",
        "    t1_stop = perf_counter()\n",
        "    print(\"Elapsed time (seconds):\", t1_stop-t1_start)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lD8JIezmwHrQ"
      },
      "outputs": [],
      "source": [
        "#@title Create orthogonal dataset .csv (<1min)\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def parse_args():\n",
        "    parser=argparse.ArgumentParser(prog='merge-dbs.py', conflict_handler='resolve')\n",
        "    parser.add_argument('-db', type=str, required=True, help='=> path/to/main_db.csv')\n",
        "    parser.add_argument('-db_dir', type=str, required=True, help='=> path/to/database_directory')\n",
        "    parser.add_argument('-o', type=str, required=True, help='=> path/to/merged_outfile.csv')\n",
        "    return(parser.parse_args())\n",
        "\n",
        "def glob_files(path: str) -> list[str]:\n",
        "    return(glob.glob(f'{path}/*'))\n",
        "\n",
        "def auto_merge(in_df: pd.DataFrame, glob_files: list[str]) -> tuple[pd.DataFrame, list[str]]:\n",
        "    leftovers=[]\n",
        "    for file in glob_files:\n",
        "        # Create dataframe with selected data from each compatible db files in loop\n",
        "        filename=file.split('/')[-1]\n",
        "        db=pd.read_csv(file)\n",
        "        cols=list(db)\n",
        "        col_intersect=list(in_df.columns.intersection(db.columns)) # get key col, assume one\n",
        "\n",
        "        # If shared key col exists, unpack to unique var, else skip/report file error\n",
        "        if col_intersect:\n",
        "            cols.insert(0, cols.pop(cols.index(col_intersect[0]))) # move intersected key col to the front\n",
        "            db=db.loc[:, cols]\n",
        "            sharedID, *othercols=db.columns\n",
        "            print(f'{filename}: joined at {sharedID}')\n",
        "        else:\n",
        "            print(f'{filename}: Missing shared key column')\n",
        "            leftovers.append(file)\n",
        "            continue\n",
        "        # Merge cleaned up dfs\n",
        "        db.drop_duplicates(subset=sharedID, keep='first', inplace=True)\n",
        "        in_df=pd.merge(in_df, db[[sharedID, *othercols]],on=sharedID, how='left')\n",
        "    return(in_df, leftovers)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Appends column-specific data from other .csv files to a designated .csv file\n",
        "    \"\"\"\n",
        "    args=parse_args()\n",
        "\n",
        "    # Set main db file\n",
        "    db_csv=pd.read_csv(args.db)\n",
        "\n",
        "    # Set directory of db files to merge\n",
        "    db_files=glob_files(args.db_dir.rstrip('/'))\n",
        "\n",
        "    # Merge each db_file to in_csv by shared key col, record unmerged files\n",
        "    db_csv, db_leftovers=auto_merge(db_csv, db_files)\n",
        "    \n",
        "    # Retry with unmerged files, if applicable, then export .csv\n",
        "    if db_leftovers:\n",
        "        db_csv, remaining_leftovers=auto_merge(db_csv, db_leftovers)\n",
        "        db_csv.to_csv(args.o, index=False, mode='w', header=True)\n",
        "        if remaining_leftovers:\n",
        "            remaining_filenames=[file.split('/')[-1] for file in remaining_leftovers]\n",
        "            print(f'Remaining un-merged files: {remaining_filenames}')\n",
        "    else:\n",
        "        db_csv.to_csv(args.o, index=False, mode='w', header=True)\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VKUw5aR4vmbh"
      },
      "outputs": [],
      "source": [
        "#@title Merge orthogonal dataset to .csv summary file (<1min)\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "def parse_args():\n",
        "    parser=argparse.ArgumentParser(prog='merge-to-db.py', conflict_handler='resolve')\n",
        "    parser.add_argument('-i', type=str, required=True, help='=> path/to/infile.csv')\n",
        "    parser.add_argument('-db', type=str, required=True, help='=> path/to/main_db')\n",
        "    parser.add_argument('-keycolumn', type=str, required=True, help='=> key column to merge on')\n",
        "    parser.add_argument('-o', type=str, required=True, help='=> path/to/merged_outfile.csv')\n",
        "    return(parser.parse_args())\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Appends column-specific data from a db.csv file to a designated .csv file\n",
        "    \"\"\"\n",
        "    args=parse_args()\n",
        "\n",
        "    # Set main input df to merge file\n",
        "    in_df=pd.read_csv(args.i)\n",
        "\n",
        "    # Set main db file and columns to merge using config (.ini)\n",
        "    db_df=pd.read_csv(args.db)\n",
        "    keycol=args.keycolumn\n",
        "    # Re-order db_df and merge to input df on keycol val\n",
        "    sID_col = db_df.pop(keycol)\n",
        "    db_df.insert(0, sID_col.name, sID_col)\n",
        "    sID, *othercols = db_df.columns\n",
        "    in_df=pd.merge(in_df, db_df[[sID, *othercols]],on=keycol, how='left')\n",
        "\n",
        "    # Hard-coded, this order doesn't need to change\n",
        "    final_cols_order = ['sequenceID', 'Gene_Sym', 'description', 'AA_seqlength', 'start', 'count', 'Num_Unique', 'concat_sites', 'org_pvals', 'Non_hits', 'best_pval', 'human_site', 'pval_hg38', 'FUBAR_PSRs', 'Resource_Plate', 'Resource_Position', 'hORF_Length', 'PC1', 'Omega', 'calc_AF', 'log_calc_AF', 'human_hit', 'Ifn_u2', 'Ifn_u5', 'Ifn_d2', 'Ifn_d5']\n",
        "    in_df=in_df.loc[:, final_cols_order]\n",
        "    in_df.to_csv(args.o, index=False, mode='w', header=True)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A2CoKKAEv76z"
      },
      "outputs": [],
      "source": [
        "#@title Package and download results\n",
        "#@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "# if msa_mode == \"custom\":\n",
        "#   print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "\n",
        "# !zip -FSr $jobname\".result.zip\" config.json $jobname*\".json\" $jobname*\".a3m\" $jobname*\"relaxed_rank_\"*\".pdb\" \"cite.bibtex\" $jobname*\".png\"\n",
        "# files.download(f\"{jobname}.result.zip\")\n",
        "\n",
        "# if save_to_google_drive == True and drive:\n",
        "#   uploaded = drive.CreateFile({'title': f\"{jobname}.result.zip\"})\n",
        "#   uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
        "#   uploaded.Upload()\n",
        "#   print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SLiME.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
